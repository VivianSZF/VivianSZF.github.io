<!doctype html>
<html lang="en">


<!-- === Header Starts === -->
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>DepthGAN</title>

  <link href="./assets/bootstrap.min.css" rel="stylesheet">
  <link href="./assets/font.css" rel="stylesheet" type="text/css">
  <link href="./assets/style.css" rel="stylesheet" type="text/css">
</head>
<!-- === Header Ends === -->


<body>


<!-- === Home Section Starts === -->
<div class="section">
  <!-- === Title Starts === -->
  <div class="header">
    <div class="title", style="padding-top: 25pt;">
      3D-Aware Indoor Scene Synthesis with Depth Priors
    </div>
  </div>
  <!-- === Title Ends === -->
  <div class="author">
    <a href="https://shizifan.github.io/" target="_blank">Zifan Shi</a><sup>1</sup>,&nbsp;
    <a href="http://shenyujun.github.io/" target="_blank">Yujun Shen</a><sup>2</sup>,&nbsp;
    <a href="https://zhujiapeng.github.io/" target="_blank">Jiapeng Zhu</a><sup>1</sup>,&nbsp;
    <a href="https://sites.google.com/view/dyyeung" target="_blank">Dit-Yan Yeung</a><sup>1</sup>,&nbsp;
    <a href="https://cqf.io/" target="_blank">Qifeng Chen</a><sup>1</sup>

  </div>
  <div class="institution">
    <sup>1</sup>Hong Kong University of Science and Technology &nbsp; &nbsp; &nbsp; &nbsp;
    <sup>2</sup>ByteDance Inc.
  </div>
  <div class="link">
    [<a href="https://arxiv.org/pdf/TODO" target="_blank">Paper</a>]
    [<a href="https://github.com/VivianSZF/DepthGAN" target="_blank">Code</a>]
    [<a href="https://youtu.be/RMmIso5Oxno" target="_blank">Demo</a>]
  </div>
  <div class="teaser">
    <img src="./assets/teaser.jpg">
  </div>
  <div class="body">
    <b>Figure:</b> Left: High-fidelity images with the corresponding depths generated by DepthGAN.
    Right: The 3D reconstruction results.
  </div>
</div>
<!-- === Home Section Ends === -->


<!-- === Overview Section Starts === -->
<div class="section">
  <div class="title">Overview</div>
  <div class="body">
    Existing methods fail to learn high-fidelity 3D-aware indoor scene synthesis merely 
    from 2D images. In this work, we fill in this gap by proposing <b>DepthGAN</b>, which 
    incorporates depth as a 3D prior. Concretely, we propose a <i>dual-path generator</i>, where 
    one path is responsible for depth generation, whose intermediate features are injected 
    into the other path as the condition for appearance rendering. Such a design eases the 
    3D-aware synthesis with explicit geometry information. Meanwhile, we introduce 
    a <i>switchable discriminator</i> both to differentiate real <i>v.s.</i> fake domains, and 
    to predict the depth from a given input. In this way, the discriminator can take the 
    spatial arrangement into account and advise the generator to learn an appropriate depth 
    condition. Extensive experimental results suggest that our approach is capable of 
    synthesizing indoor scenes with impressively good quality and 3D consistency, significantly 
    outperforming state-of-the-art alternatives.
  </div>
</div>
<!-- === Overview Section Ends === -->


<!-- === Result Section Starts === -->
<div class="section">
  <div class="title">Results</div>
  <div class="body">

    Qualitative comparison between DepthGAN and existing alternatives.

    <table width="100%" style="margin: 20pt 0; text-align: center;">
      <tr>
        <td><img src="./assets/result1.jpg" width="90%"></td>
      </tr>
    </table>

    Diverse synthesis via varying the appearance latent code, conditioned on the same depth image.

    <table width="100%" style="margin: 20pt 0; text-align: center;">
      <tr>
        <td><img src="./assets/depth_fixed.jpg" width="60%"></td>
      </tr>
    </table>

    Diverse geometries via varying the depth latent code, rendered with the same appearance style.

    <table width="100%" style="margin: 20pt 0; text-align: center;">
      <tr>
        <td><img src="./assets/appearance_fixed.jpg" width="60%"></td>
      </tr>
    </table>
  </div>
</div>
<!-- === Result Section Ends === -->

<!-- === Demo video Section Starts === -->
<div class="section">
  <div class="title">Demo</div>
  <div class="body">

    <p style="margin-top: 20pt"><a name="demo"></a></p>
    We include a demo video, which shows the continuous 3D control achieved by our DepthGAN.
    <div style="position: relative; padding-top: 50%; margin: 20pt auto; text-align: center;">
      <iframe src="https://www.youtube.com/embed/RMmIso5Oxno" frameborder=0
              style="position: absolute; top: 2.5%; left: 2.5%; width: 95%; height: 100%;"
              allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"
              allowfullscreen></iframe>
    </div>

  </div>
</div>
<!-- === Demo Video Section Ends === -->


<!-- === Reference Section Starts === -->
<div class="section">
  <div class="bibtex">BibTeX</div>
<pre>
  @article{....,
    title     = {3D-Aware Indoor Scene Synthesis with Depth Priors},
    author    = {Shi, Zifan and Shen, Yujun and Zhu, Jiapeng and Yeung, Dit-Yan and Chen, Qifeng},
    article   = {},
    year      = {2022}
  }

</pre>


<div class="ref">Related Work</div>

<div class="citation">
  <div class="image"><img src="./assets/hologan.jpg"></div>
  <div class="comment">
    <a href="https://arxiv.org/pdf/1904.01326.pdf" target="_blank">
      Thu Nguyen-Phuoc, Chuan Li, Lucas Theis, Christian Richardt, Yong-Liang Yang.
      HoloGAN: Unsupervised learning of 3D representations from natural images.
      ICCV, 2019.</a><br>
    <b>Comment:</b>
    Proposes voxelized and implicit 3D representations and then render it to 2D image space with a reshape operation.
  </div>
</div>


<div class="citation">
  <div class="image"><img src="./assets/graf.jpg"></div>
  <div class="comment">
    <a href="https://arxiv.org/pdf/2007.02442.pdf" target="_blank">
      Katja Schwarz, Yiyi Liao, Michael Niemeyer, Andreas Geiger.
      GRAF: Generative Radiance Fields for 3D-Aware Image Synthesis.
      NeurIPS, 2020.</a><br>
    <b>Comment:</b>
    Proposes the generative radiance fields for 3D-aware image synthesis.
  </div>
</div>

<div class="citation">
  <div class="image"><img src="./assets/giraffe.jpg"></div>
  <div class="comment">
    <a href="https://arxiv.org/pdf/2011.12100.pdf" target="_blank">
      Michael Niemeyer, Andreas Geiger.
      GIRAFFE: Representing Scenes as Compositional Generative Neural Feature Fields.
      CVPR, 2021.</a><br>
    <b>Comment:</b>
    Proposes the compositional generative neural feature fields for scene synthesis.
  </div>
</div>

<div class="citation">
  <div class="image"><img src="./assets/pigan.jpg"></div>
  <div class="comment">
    <a href="https://arxiv.org/pdf/2012.00926.pdf" target="_blank">
      Eric R. Chan, Marco Monteiro, Petr Kellnhofer, Jiajun Wu, Gordon Wetzstein.
      pi-GAN: Periodic Implicit Generative Adversarial Networks for 3D-Aware Image Synthesis.
      CVPR, 2021.</a><br>
    <b>Comment:</b>
    Proposes the periodic implicit generative neural feature fields for 3d-aware image synthesis.
  </div>
</div>

<div class="citation">
  <div class="image"><img src="./assets/rgbd-gan.jpg"></div>
  <div class="comment">
    <a href="https://arxiv.org/pdf/1909.12573.pdf" target="_blank">
      Atsuhiro Noguchi, Tatsuya Harada. 
      RGBD-GAN: Unsupervised 3D Representation Learning From Natural Image Datasets via RGBD Image Synthesis.
      ICLR, 2020.</a><br>
    <b>Comment:</b>
    Proposes the unsupervised depth learning for 3d-aware image synthesis.
  </div>
</div>

<!-- === Reference Section Ends === -->


</body>
</html>
